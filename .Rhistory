confusionMatrix(pred_rf, vowel.test$y)$overall[1]
#Extract accuracies for gbm
confusionMatrix(pred_gbp, vowel.test$y)$overall[1]
predDF <- data.frame(pred_rf, pred_gbp, y=vowel.test$y)
sum(pred_rf[predDF$pred_rf == predDF$pred_gbm] ==
predDF$y[predDF$pred_rf == predDF$pred_gbm]) /
sum(predDF$pred_rf == predDF$pred_gbm)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train
vowel.test
set.seed(33833)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
library(caret)
#Fit a RF predictor relating the factor variable y to the remaining variables
#using the train() from caret
mod_rf <- train(y~., data=vowel.train, method="rf")
#Fit a boosted predictor using the gbm method
#using the train() from caret
mod_gbp <- train(y~., data=vowel.train, method="gbm")
#Generate the predictions
pred_rf <-predict(mod_rf, vowel.test)
pred_gbm <- predict(mod_gbp, vowel.test)
predDF <- data.frame(pred_rf, pred_gbm, y=vowel.test$y)
sum(pred_rf[predDF$pred_rf == predDF$pred_gbm] ==
predDF$y[predDF$pred_rf == predDF$pred_gbm]) /
sum(predDF$pred_rf == predDF$pred_gbm)
dat = read.csv("C:/Users/Katelyn/Downloads/gaData.csv")
training = dat[year(dat$date) < 2012,]
library(lubridate)
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
?bats
library(forecast)
install.packages("forecast")
library(forecast)
?bats
#Question 4
library(lubridate)
dat = read.csv("C:/Users/Katelyn/Downloads/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
View(testing)
View(training)
tstrain = ts(training$visitsTumblr)
library(forecast)
mod_ts <- bats(tstrain)
fcast <- forecast(mod_ts, level=95, h=dim(testing)[1])
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
library(caret)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
library(e1071)
svm <- train(CompressiveStrength~., data = training)
?read.table.url
?read.table
training_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testing_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#Read the data from the URLs:
training <- read.table.url(training_data_URL, header= TRUE, row.names = 1)
testing <- read.table.url(testing_data_URL, header = TRUE, row.names = 1)
?read.table.url
?read.table
training <- read.table(training_data_URL, header= TRUE, row.names = 1)
training <- read.csv(url(training_data_URL), header= TRUE, row.names = 1)
?download.file
download.file(training_data_URL, "./pml-training.csv")
download.file(testing_data_URL, "./pml-testing.csv")
getwd()
training_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testing_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
file.exists("./pml-training.csv")
download.file(training_data_URL, "./pml-training.csv")
training_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testing_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#Download the files from the URLs, save them in your current working directory,
#If they are not already there
download.file(training_data_URL, "./pml-training.csv")
download.file(testing_data_URL, "./pml-testing.csv")
!file.exists("./pml-testing.csv")
training <- read.csv("./pml-training.csv", header= TRUE, row.names = 1)
View(training)
?read.csv
testing <- read.csv("./pml-testing.csv", header = TRUE, row.names = 1)
View(testing)
training_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testing_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#Download the files from the URLs, and save them in your current working directory,
#If they are not already there
if(!file.exists("./pml-training.csv")){
download.file(training_data_URL, "./pml-training.csv")
}
if(!file.exists("./pml-testing.csv")){
download.file(testing_data_URL, "./pml-testing.csv")
}
#Read the data
training <- read.csv("./pml-training.csv", header= TRUE, row.names = 1, na.strings="NA")
testing <- read.csv("./pml-testing.csv", header = TRUE, row.names = 1, na.strings="NA")
View(testing)
View(training)
#The below are the URLs provided:
training_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testing_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#Download the files from the URLs, and save them in your current working directory,
#If they are not already there
if(!file.exists("./pml-training.csv")){
download.file(training_data_URL, "./pml-training.csv")
}
if(!file.exists("./pml-testing.csv")){
download.file(testing_data_URL, "./pml-testing.csv")
}
#Read the data
training <- read.csv("./pml-training.csv", header= TRUE, row.names = 1, na.strings=c("NA",""))
testing <- read.csv("./pml-testing.csv", header = TRUE, row.names = 1, na.strings=c("NA",""))
sum(is.na(training))
sum(is.na(training[,c(1)]))
sum(is.na(training[,c(2)]))
sum(is.na(training[,3]))
sum(is.na(training[,c("max_yaw_belt")]))
cntNAs <- function(x) {
as.vector(apply(x, 2, function(x) length(which(is.na(x)))))
}
countNAs <- function(x) {
as.vector(apply(x, 2, function(x) length(which(is.na(x)))))
}
ncol(testing)
#The below are the URLs provided:
training_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testing_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#Download the files from the URLs, and save them in your current working directory,
#If they are not already there
if(!file.exists("./pml-training.csv")){
download.file(training_data_URL, "./pml-training.csv")
}
if(!file.exists("./pml-testing.csv")){
download.file(testing_data_URL, "./pml-testing.csv")
}
#Read the data
training <- read.csv("./pml-training.csv", header= TRUE, row.names = 1, na.strings=c("NA",""))
testing <- read.csv("./pml-testing.csv", header = TRUE, row.names = 1, na.strings=c("NA",""))
#Define a function to count the number of NAs in each column
countNAs <- function(x) {
as.vector(apply(x, 2, function(x) length(which(is.na(x)))))
}
number_of_NAs_per_column <- countNAs(training)
number_of_NAs_per_column
sum(is.na(training[,12]))
19216/19622
95_percent_threshold <- .95*(nrow(training))
ninetyfive_percent_threshold <- .95*(nrow(training))
.95*19622
column_names_to_drop <- c()
number_of_NAs_per_column <- countNAs(training)
number_of_NAs_per_column
length(number_of_NAs_per_column)
columns_to_drop <- number_of_NAs_per_column > .95*(nrow(training))
View(columns_to_drop)
View(columns_to_drop)
?training
cleaned_training <- training[,!columns_to_drop]
cleaned_testing <- testing[,!columns_to_drop]
View(cleaned_testing)
View(cleaned_training)
View(cleaned_testing)
cleaned_training <- cleaned_training[,-c(1,2,3,4,5,6,7)]
cleaned_testing <- cleaned_testing[,-c(1,2,3,4,5,6,7)]
View(cleaned_testing)
View(training)
#The below are the URLs provided:
training_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testing_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#Download the files from the URLs, and save them in your current working directory,
#If they are not already there
if(!file.exists("./pml-training.csv")){
download.file(training_data_URL, "./pml-training.csv")
}
if(!file.exists("./pml-testing.csv")){
download.file(testing_data_URL, "./pml-testing.csv")
}
#Read the data
training <- read.csv("./pml-training.csv", header= TRUE, row.names = 1, na.strings=c("NA",""))
testing <- read.csv("./pml-testing.csv", header = TRUE, row.names = 1, na.strings=c("NA",""))
#Define a function to count the number of NAs in each column
countNAs <- function(x) {
as.vector(apply(x, 2, function(x) length(which(is.na(x)))))
}
#For each column in the training dataset, apply the function
number_of_NAs_per_column <- countNAs(training)
#If more than 95% of the column contains NAs,
#remove the column from the training and testing data
columns_to_drop <- number_of_NAs_per_column > .95*(nrow(training))
cleaned_training <- training[,!columns_to_drop]
cleaned_testing <- testing[,!columns_to_drop]
#Also, we can drop the first 6 columns because they are not necessary for predicting
cleaned_training <- cleaned_training[,-c(1,2,3,4,5,6)]
cleaned_testing <- cleaned_testing[,-c(1,2,3,4,5,6)]
set.seed(1)
inTraining <- createDataPartition(training$classe, p=.7, list=FALSE)
library(caret)
library(caret)
set.seed(1)
training_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testing_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#Download the files from the URLs, and save them in your current working directory,
#If they are not already there
if(!file.exists("./pml-training.csv")){
download.file(training_data_URL, "./pml-training.csv")
}
if(!file.exists("./pml-testing.csv")){
download.file(testing_data_URL, "./pml-testing.csv")
}
#Read the data
training <- read.csv("./pml-training.csv", header= TRUE, row.names = 1, na.strings=c("NA",""))
testing <- read.csv("./pml-testing.csv", header = TRUE, row.names = 1, na.strings=c("NA",""))
#Define a function to count the number of NAs in each column
countNAs <- function(x) {
as.vector(apply(x, 2, function(x) length(which(is.na(x)))))
}
#For each column in the training dataset, apply the function
number_of_NAs_per_column <- countNAs(training)
#If more than 95% of the column contains NAs,
#remove the column from the training and testing data
columns_to_drop <- number_of_NAs_per_column > .95*(nrow(training))
cleaned_training <- training[,!columns_to_drop]
cleaned_testing <- testing[,!columns_to_drop]
#Also, we can drop the first 6 columns because they are not necessary for predicting
cleaned_training <- cleaned_training[,-c(1,2,3,4,5,6)]
cleaned_testing <- cleaned_testing[,-c(1,2,3,4,5,6)]
```
inTraining <- createDataPartition(training$classe, p=.7, list=FALSE)
13737/19622
new_training_set <- training[inTraining,]
new_testing_set <- training[-inTraining,]
ids_in_dataset1 <- createDataPartition(cleaned_training$classe, p = 0.33, list=FALSE)
6479/19622
remaining_training <- training[-ids_in_dataset1,]
library(caret)
set.seed(1)
training_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testing_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#Download the files from the URLs, and save them in your current working directory,
#If they are not already there
if(!file.exists("./pml-training.csv")){
download.file(training_data_URL, "./pml-training.csv")
}
if(!file.exists("./pml-testing.csv")){
download.file(testing_data_URL, "./pml-testing.csv")
}
#Read the data
training <- read.csv("./pml-training.csv", header= TRUE, row.names = 1, na.strings=c("NA",""))
testing <- read.csv("./pml-testing.csv", header = TRUE, row.names = 1, na.strings=c("NA",""))
#Define a function to count the number of NAs in each column
countNAs <- function(x) {
as.vector(apply(x, 2, function(x) length(which(is.na(x)))))
}
#For each column in the training dataset, apply the function
number_of_NAs_per_column <- countNAs(training)
#If more than 95% of the column contains NAs,
#remove the column from the training and testing data
columns_to_drop <- number_of_NAs_per_column > .95*(nrow(training))
cleaned_training <- training[,!columns_to_drop]
cleaned_testing <- testing[,!columns_to_drop]
#Also, we can drop the first 6 columns because they are not necessary for predicting
cleaned_training <- cleaned_training[,-c(1,2,3,4,5,6)]
cleaned_testing <- cleaned_testing[,-c(1,2,3,4,5,6)]
#Create data set 1
ids_in_dataset1 <- createDataPartition(cleaned_training$classe, p = 0.33, list=FALSE)
dataset1 <- training[ids_in_dataset1,]
remaining_training <- training[-ids_in_dataset1,]
#Create data set 2
ids_in_dataset2 <- createDataPartition(remaining_training$classe, p = 0.5, list=FALSE)
dataset2 <- training[ids_in_dataset2,]
dataset3 <- training[-ids_in_dataset2,]
nrow(dataset1)
nrow(dataset2)
nrow(dataset3)
library(caret)
set.seed(1)
#The below are the URLs provided:
training_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testing_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#Download the files from the URLs, and save them in your current working directory,
#If they are not already there
if(!file.exists("./pml-training.csv")){
download.file(training_data_URL, "./pml-training.csv")
}
if(!file.exists("./pml-testing.csv")){
download.file(testing_data_URL, "./pml-testing.csv")
}
#Read the data
training <- read.csv("./pml-training.csv", header= TRUE, row.names = 1, na.strings=c("NA",""))
testing <- read.csv("./pml-testing.csv", header = TRUE, row.names = 1, na.strings=c("NA",""))
#Define a function to count the number of NAs in each column
countNAs <- function(x) {
as.vector(apply(x, 2, function(x) length(which(is.na(x)))))
}
#For each column in the training dataset, apply the function
number_of_NAs_per_column <- countNAs(training)
#If more than 95% of the column contains NAs,
#remove the column from the training and testing data
columns_to_drop <- number_of_NAs_per_column > .95*(nrow(training))
cleaned_training <- training[,!columns_to_drop]
cleaned_testing <- testing[,!columns_to_drop]
#Also, we can drop the first 6 columns because they are not necessary for predicting
cleaned_training <- cleaned_training[,-c(1,2,3,4,5,6)]
cleaned_testing <- cleaned_testing[,-c(1,2,3,4,5,6)]
#Create data set 1
ids_in_dataset1 <- createDataPartition(cleaned_training$classe, p = 0.33, list=FALSE)
dataset1 <- training[ids_in_dataset1,]
remaining_training <- training[-ids_in_dataset1,]
#Create data set 2 and 3
ids_in_dataset2 <- createDataPartition(remaining_training$classe, p = 0.5, list=FALSE)
dataset2 <- remaining_training[ids_in_dataset2,]
dataset3 <- remaining_training[-ids_in_dataset2,]
#Check data sets 1, 2, and 3 are all approximately the same size
nrow(dataset1)
nrow(dataset2)
nrow(dataset3)
6479/19622
6571/19622
6572+6479+6571
inTraining1 <- createDataPartition(dataset1$classe, p=.6, list=FALSE)
#Split data set 1 into training and testing
inTraining1 <- createDataPartition(dataset1$classe, p=.6, list=FALSE)
training1 <- dataset1[inTraining1,]
testing1 <- dataset1[-inTraining1,]
#Split data set 2 into training and testing
inTraining2 <- createDataPartition(dataset2$classe, p=.6, list=FALSE)
training2 <-dataset2[inTraining2,]
testing2 <- dataset2[-inTraining2,]
#Split data set 3 into training and testing
inTraining3<- createDataPartition(dataset2$classe, p=.6, list=FALSE)
training3 <-dataset3[inTraining3,]
testing3 <-dataset3[-inTraining3,]
library(caret)
set.seed(1)
#The below are the URLs provided:
training_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testing_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#Download the files from the URLs, and save them in your current working directory,
#If they are not already there
if(!file.exists("./pml-training.csv")){
download.file(training_data_URL, "./pml-training.csv")
}
if(!file.exists("./pml-testing.csv")){
download.file(testing_data_URL, "./pml-testing.csv")
}
#Read the data
training <- read.csv("./pml-training.csv", header= TRUE, row.names = 1, na.strings=c("NA",""))
testing <- read.csv("./pml-testing.csv", header = TRUE, row.names = 1, na.strings=c("NA",""))
#Define a function to count the number of NAs in each column
countNAs <- function(x) {
as.vector(apply(x, 2, function(x) length(which(is.na(x)))))
}
#For each column in the training dataset, apply the function
number_of_NAs_per_column <- countNAs(training)
#If more than 95% of the column contains NAs,
#remove the column from the training and testing data
columns_to_drop <- number_of_NAs_per_column > .95*(nrow(training))
cleaned_training <- training[,!columns_to_drop]
cleaned_testing <- testing[,!columns_to_drop]
#Also, we can drop the first 6 columns because they are not necessary for predicting
cleaned_training <- cleaned_training[,-c(1,2,3,4,5,6)]
cleaned_testing <- cleaned_testing[,-c(1,2,3,4,5,6)]
#Create data set 1
ids_in_dataset1 <- createDataPartition(cleaned_training$classe, p = 0.33, list=FALSE)
dataset1 <- cleaned_training[ids_in_dataset1,]
remaining_training <- cleaned_training[-ids_in_dataset1,]
#Create data set 2 and 3
ids_in_dataset2 <- createDataPartition(remaining_training$classe, p = 0.5, list=FALSE)
dataset2 <- remaining_training[ids_in_dataset2,]
dataset3 <- remaining_training[-ids_in_dataset2,]
#Split data set 1 into training and testing
inTraining1 <- createDataPartition(dataset1$classe, p=.6, list=FALSE)
training1 <- dataset1[inTraining1,]
testing1 <- dataset1[-inTraining1,]
#Split data set 2 into training and testing
inTraining2 <- createDataPartition(dataset2$classe, p=.6, list=FALSE)
training2 <-dataset2[inTraining2,]
testing2 <- dataset2[-inTraining2,]
#Split data set 3 into training and testing
inTraining3<- createDataPartition(dataset2$classe, p=.6, list=FALSE)
training3 <-dataset3[inTraining3,]
testing3 <-dataset3[-inTraining3,]
model1_rf <- train(training1$classe ~ .,
method="rf",
preProcess=c("center", "scale"),
trControl=trainControl(method = "cv", number = 3),
data=training1)
model2_gbm <- train(training2$classe ~ .,
method="gbm",
preProcess=c("center", "scale"),
trControl=trainControl(method = "cv", number = 3),
data=training2)
model3_lda <- train(training3$classe ~ .,
method="lda",
preProcess=c("center", "scale"),
trControl=trainControl(method = "cv", number = 3),
data=training3)
print(rf_model1, digits=3)
print(model1_rf, digits=3)
print(model2_gbm, digits=3)
print(model3_lda, digits=3)
pred <- predict(model1_rf, testing1)
pred1 <- predict(model1_rf, testing1)
pred2 <- predict(model2_gbm, testing2)
pred3 <- predict(model3_lda, testing3)
print(confusionMatrix(pred1, testing1$classe), digits=3)
print(confusionMatrix(pred2, testing2$classe), digits=3)
print(confusionMatrix(pred3, testing3$classe), digits=3)
print(predict(model1_rf, newdata=cleaned_testing))
View(testing)
for(i in 1:nrow(predict(model1_rf, newdata=cleaned_testing))){print(i)
print(predict(model1_rf, newdata=cleaned_testing))
}
testing
View(testing)
library(caret)
set.seed(1)
#The below are the URLs provided:
training_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testing_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#Download the files from the URLs, and save them in your current working directory,
#If they are not already there
if(!file.exists("./pml-training.csv")){
download.file(training_data_URL, "./pml-training.csv")
}
if(!file.exists("./pml-testing.csv")){
download.file(testing_data_URL, "./pml-testing.csv")
}
#Read the data
training <- read.csv("./pml-training.csv", header= TRUE, row.names = 1, na.strings=c("NA",""))
testing <- read.csv("./pml-testing.csv", header = TRUE, row.names = 1, na.strings=c("NA",""))
#Define a function to count the number of NAs in each column
countNAs <- function(x) {
as.vector(apply(x, 2, function(x) length(which(is.na(x)))))
}
#For each column in the training dataset, apply the function
number_of_NAs_per_column <- countNAs(training)
#If more than 95% of the column contains NAs,
#remove the column from the training and testing data
columns_to_drop <- number_of_NAs_per_column > .95*(nrow(training))
cleaned_training <- training[,!columns_to_drop]
cleaned_testing <- testing[,!columns_to_drop]
#Also, we can drop the first 6 columns because they are not necessary for predicting
cleaned_training <- cleaned_training[,-c(1,2,3,4,5,6)]
cleaned_testing <- cleaned_testing[,-c(1,2,3,4,5,6)]
#Create data set 1
ids_in_dataset1 <- createDataPartition(cleaned_training$classe, p = 0.33, list=FALSE)
dataset1 <- cleaned_training[ids_in_dataset1,]
remaining_training <- cleaned_training[-ids_in_dataset1,]
#Create data set 2 and 3
ids_in_dataset2 <- createDataPartition(remaining_training$classe, p = 0.5, list=FALSE)
dataset2 <- remaining_training[ids_in_dataset2,]
dataset3 <- remaining_training[-ids_in_dataset2,]
#Split data set 1 into training and testing
inTraining1 <- createDataPartition(dataset1$classe, p=.6, list=FALSE)
training1 <- dataset1[inTraining1,]
testing1 <- dataset1[-inTraining1,]
#Split data set 2 into training and testing
inTraining2 <- createDataPartition(dataset2$classe, p=.6, list=FALSE)
training2 <-dataset2[inTraining2,]
testing2 <- dataset2[-inTraining2,]
#Split data set 3 into training and testing
inTraining3<- createDataPartition(dataset2$classe, p=.6, list=FALSE)
training3 <-dataset3[inTraining3,]
testing3 <-dataset3[-inTraining3,]
model1_rf <- train(training1$classe ~ .,
method="rf",
preProcess=c("center", "scale"),
trControl=trainControl(method = "cv", number = 3),
data=training1)
model2_gbm <- train(training2$classe ~ .,
method="gbm",
preProcess=c("center", "scale"),
trControl=trainControl(method = "cv", number = 3),
data=training2)
model3_lda <- train(training3$classe ~ .,
method="lda",
preProcess=c("center", "scale"),
trControl=trainControl(method = "cv", number = 3),
data=training3)
print(model1_rf, digits=3)
print(model2_gbm, digits=3)
print(model3_lda, digits=3)
pred1 <- predict(model1_rf, testing1)
print(confusionMatrix(pred1, testing1$classe), digits=3)
1-0.979
knit2html()
getwd()
ls
getwd()
setwd("C:/Users/Katelyn/Documents/Prac_ML_Course_Project")
getwd()
library(caret)
set.seed(1)
training_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testing_data_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#Download the files from the URLs into the current working directory
download.file(training_data_URL, "./pml-training.csv")
download.file(testing_data_URL, "./pml-testing.csv")
!file.exists("./pml-training.csv")
